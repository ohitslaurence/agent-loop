#!/usr/bin/env bash
# loop - Run claude in a loop until COMPLETE token is detected
#
# This wrapper delegates to loopctl when the loopd daemon is running,
# falling back to the built-in bash loop otherwise.

set -euo pipefail

# Resolve symlinks to find the actual script location
resolve_script_dir() {
  local source="${BASH_SOURCE[0]}"
  while [[ -L "$source" ]]; do
    local dir
    dir="$(cd -P "$(dirname "$source")" && pwd)"
    source="$(readlink "$source")"
    [[ "$source" != /* ]] && source="$dir/$source"
  done
  cd -P "$(dirname "$source")" && pwd
}

SCRIPT_DIR="$(resolve_script_dir)"
LIB_DIR="$SCRIPT_DIR/../lib"

# -----------------------------------------------------------------------------
# Daemon detection and delegation
# -----------------------------------------------------------------------------
LOOPD_ADDR="${LOOPD_ADDR:-http://127.0.0.1:7700}"
LOOPD_TOKEN="${LOOPD_TOKEN:-}"
USE_DAEMON="${LOOP_USE_DAEMON:-auto}"  # auto, yes, no

# Check if loopd daemon is reachable
is_daemon_available() {
  if [[ "$USE_DAEMON" == "no" ]]; then
    return 1
  fi
  if [[ "$USE_DAEMON" == "yes" ]]; then
    return 0
  fi
  # auto: probe the health endpoint
  if command -v curl >/dev/null 2>&1; then
    curl -sf --max-time 1 "${LOOPD_ADDR}/health" >/dev/null 2>&1
    return $?
  fi
  return 1
}

# Check if loopctl binary is available
has_loopctl() {
  command -v loopctl >/dev/null 2>&1
}

# Delegate to loopctl with translated arguments
delegate_to_loopctl() {
  local -a loopctl_args=()

  # Global options
  [[ -n "${LOOPD_ADDR:-}" ]] && loopctl_args+=(--addr "$LOOPD_ADDR")
  [[ -n "${LOOPD_TOKEN:-}" ]] && loopctl_args+=(--token "$LOOPD_TOKEN")

  # Parse and translate arguments
  local spec_path=""
  local plan_path=""
  local config_path=""
  local name=""
  local name_source=""
  local merge_target=""
  local merge_strategy=""
  local base_branch=""
  local subcommand=""

  while [[ $# -gt 0 ]]; do
    case "$1" in
      prompt)
        # 'prompt' subcommand not supported via daemon; fall back to bash
        return 1
        ;;
      --config)
        config_path="$2"
        shift 2
        ;;
      --config=*)
        config_path="${1#--config=}"
        shift
        ;;
      --init-config)
        # init-config is a local operation; fall back to bash
        return 1
        ;;
      # These flags are not supported by loopctl; fall back
      --iterations|--log-dir|--model|--completion-mode|--mode|--prompt|\
      --verify-cmd|--verify-timeout-sec|--measure-cmd|--measure-timeout-sec|\
      --claude-timeout-sec|--claude-retries|--claude-retry-backoff-sec|\
      --no-postmortem|--postmortem|--no-gum|--reviewer|--no-reviewer|\
      --summary-json|--no-wait)
        return 1
        ;;
      -h|--help)
        # Show help from bash version
        return 1
        ;;
      -*)
        # Unknown option; fall back to bash for safety
        return 1
        ;;
      *)
        # Positional arguments
        if [[ -z "$spec_path" ]]; then
          spec_path="$1"
        elif [[ -z "$plan_path" ]]; then
          plan_path="$1"
        else
          # Too many positional args; fall back
          return 1
        fi
        shift
        ;;
    esac
  done

  # Must have at least a spec to create a run
  if [[ -z "$spec_path" ]]; then
    return 1
  fi

  # Build loopctl run command
  loopctl_args+=(run "$spec_path")
  [[ -n "$plan_path" ]] && loopctl_args+=("$plan_path")
  [[ -n "$config_path" ]] && loopctl_args+=(--config "$config_path")
  [[ -n "$name" ]] && loopctl_args+=(--name "$name")
  [[ -n "$name_source" ]] && loopctl_args+=(--name-source "$name_source")
  [[ -n "$base_branch" ]] && loopctl_args+=(--base-branch "$base_branch")
  [[ -n "$merge_target" ]] && loopctl_args+=(--merge-target "$merge_target")
  [[ -n "$merge_strategy" ]] && loopctl_args+=(--merge-strategy "$merge_strategy")

  exec loopctl "${loopctl_args[@]}"
}

# Try to delegate to daemon if available
try_delegate_to_daemon() {
  if ! has_loopctl; then
    return 1
  fi
  if ! is_daemon_available; then
    return 1
  fi
  delegate_to_loopctl "$@"
}

# Attempt daemon delegation first (unless LOOP_USE_DAEMON=no)
if [[ "${USE_DAEMON}" != "no" ]]; then
  if try_delegate_to_daemon "$@"; then
    exit 0
  fi
  # If delegation returns non-zero, fall through to bash implementation
fi

# Source UI helpers
# shellcheck source=../lib/agent-loop-ui.sh
source "$LIB_DIR/agent-loop-ui.sh"

# shellcheck source=../lib/spec-picker.sh
source "$LIB_DIR/spec-picker.sh"

# -----------------------------------------------------------------------------
# Defaults
# -----------------------------------------------------------------------------
spec_path=""
plan_path=""
specs_dir="specs"
plans_dir="specs/planning"
iterations=50
log_dir="logs/loop"
no_gum=false
summary_json=true
no_wait=false
model="opus"
postmortem=true
completion_mode="trailing"
mode="plan"
config_path="${LOOP_CONFIG:-}"
init_config=false
show_prompt=false
prompt_file=""
context_files=""
reviewer=true
PROJECT_ROOT=""

# Verification / resiliency
verify_cmds=""
verify_timeout_sec=0
measure_cmd=""
measure_timeout_sec=0
claude_timeout_sec=0
claude_retries=0
claude_retry_backoff_sec=5

# Internal state
RUNNER_NOTES_PATH=""
LAST_VERIFIED_HEAD=""
EXPERIMENT_LOG_PATH=""
EXPERIMENT_SUMMARY_DIR=""
EXPERIMENT_METRICS_DIR=""

# -----------------------------------------------------------------------------
# Usage
# -----------------------------------------------------------------------------
usage() {
  cat <<EOF
Usage: $(basename "$0") [command] [spec-path] [plan-path] [options]

Commands:
  prompt              Show the prompt that would be sent to Claude (without running)
  (default)           Run the agent loop

Arguments:
  spec-path           Path to spec file (optional if gum available)
  plan-path           Path to plan file (defaults to <plans_dir>/<spec>-plan.md)

Options:
  --iterations <n>    Maximum loop iterations (default: 50)
  --log-dir <path>    Base log directory (default: logs/loop)
  --model <name>      Claude model or alias (default: opus)
  --completion-mode   Completion detection (exact|trailing, default: trailing)
  --mode <name>       Run mode (plan|experiment, default: plan)
  --prompt <path>     Custom prompt file (default: .loop/prompt.txt if exists)
  --verify-cmd <cmd>  Verification command (repeatable). Also configurable via verify_cmds in config.
  --verify-timeout-sec <n>  Timeout per verification command (0 = none)
  --measure-cmd <cmd> Measurement command (experiment mode). Writes to LOOP_METRICS_OUT.
  --measure-timeout-sec <n>  Timeout per measurement command (0 = none)
  --claude-timeout-sec <n>  Timeout per Claude iteration (0 = none)
  --claude-retries <n>       Retries per iteration on non-zero exit (default: 0)
  --claude-retry-backoff-sec <n>  Sleep between retries (default: 5)
  --no-postmortem     Disable automatic post-run analysis
  --no-reviewer       Disable reviewer phase after each implementation
  --reviewer          Enable reviewer phase (default: enabled)
  --no-gum            Disable gum UI, use plain output
  --summary-json      Write summary JSON at end of run (default: enabled)
  --no-wait           Skip completion screen wait
  --config <path>     Load config file (overrides project config)
  --init-config       Create a project config file and exit

Config (project local):
  .loop/config (resolved from git root or current directory)
  .loop/prompt.txt (custom prompt template)

Config keys:
  specs_dir, plans_dir, log_dir, model, iterations, completion_mode, mode,
  postmortem, summary_json, no_wait, no_gum, prompt_file, context_files, reviewer,
  verify_cmds, verify_timeout_sec, measure_cmd, measure_timeout_sec,
  claude_timeout_sec, claude_retries, claude_retry_backoff_sec

Context files:
  Space-separated list of files to include as @path references in prompt.
  Example: context_files="specs/README.md specs/planning/SPEC_AUTHORING.md"

Examples:
  $(basename "$0") specs/my-feature.md
  $(basename "$0") prompt specs/my-feature.md
  $(basename "$0") specs/my-feature.md specs/planning/my-feature-plan.md --iterations 10
  $(basename "$0") --init-config
EOF
}

# -----------------------------------------------------------------------------
# Config loading (safe key=value parsing)
# -----------------------------------------------------------------------------
trim_whitespace() {
  local value="$1"
  value="${value#"${value%%[!$' \t\n\r']*}"}"
  value="${value%"${value##*[!$' \t\n\r']}"}"
  printf '%s' "$value"
}

normalize_bool() {
  case "$1" in
    true|false)
      printf '%s' "$1"
      ;;
    1|yes|y|on)
      printf 'true'
      ;;
    0|no|n|off)
      printf 'false'
      ;;
    *)
      printf '%s' "$1"
      ;;
  esac
}

apply_config_value() {
  local key="$1"
  local value="$2"
  local source="$3"

  case "$key" in
    specs_dir)
      specs_dir="$value"
      ;;
    plans_dir)
      plans_dir="$value"
      ;;
    log_dir)
      log_dir="$value"
      ;;
    model)
      model="$value"
      ;;
    iterations)
      iterations="$value"
      ;;
    completion_mode)
      completion_mode="$value"
      ;;
    mode)
      mode="$value"
      ;;
    prompt_file)
      prompt_file="$value"
      ;;
    context_files)
      context_files="$value"
      ;;
    verify_cmds)
      verify_cmds="$value"
      ;;
    verify_timeout_sec)
      verify_timeout_sec="$value"
      ;;
    measure_cmd)
      measure_cmd="$value"
      ;;
    measure_timeout_sec)
      measure_timeout_sec="$value"
      ;;
    claude_timeout_sec)
      claude_timeout_sec="$value"
      ;;
    claude_retries)
      claude_retries="$value"
      ;;
    claude_retry_backoff_sec)
      claude_retry_backoff_sec="$value"
      ;;
    postmortem|summary_json|no_wait|no_gum|reviewer)
      local normalized
      normalized=$(normalize_bool "$value")
      if [[ "$normalized" != "true" && "$normalized" != "false" ]]; then
        echo "Warning: invalid boolean for $key in $source: $value" >&2
        return 0
      fi
      case "$key" in
        postmortem) postmortem="$normalized" ;;
        summary_json) summary_json="$normalized" ;;
        no_wait) no_wait="$normalized" ;;
        no_gum) no_gum="$normalized" ;;
        reviewer) reviewer="$normalized" ;;
      esac
      ;;
    *)
      echo "Warning: unknown config key in $source: $key" >&2
      ;;
  esac
}

load_config_file() {
  local path="$1"
  local required="${2:-false}"

  if [[ ! -f "$path" ]]; then
    if [[ "$required" == "true" ]]; then
      echo "Error: Config file not found: $path" >&2
      exit 1
    fi
    return 0
  fi

  while IFS= read -r line || [[ -n "$line" ]]; do
    local trimmed
    trimmed=$(trim_whitespace "$line")
    if [[ -z "$trimmed" || "$trimmed" == \#* ]]; then
      continue
    fi

    if [[ "$trimmed" != *"="* ]]; then
      echo "Warning: invalid config line in $path: $line" >&2
      continue
    fi

    local key="${trimmed%%=*}"
    local value="${trimmed#*=}"
    key=$(trim_whitespace "$key")
    value=$(trim_whitespace "$value")

    if [[ -z "$key" ]]; then
      echo "Warning: invalid config line in $path: $line" >&2
      continue
    fi

    if [[ ${#value} -ge 2 && ${value:0:1} == '"' && ${value: -1} == '"' ]]; then
      value="${value:1:-1}"
    elif [[ ${#value} -ge 2 && ${value:0:1} == "'" && ${value: -1} == "'" ]]; then
      value="${value:1:-1}"
    fi

    apply_config_value "$key" "$value" "$path"
  done < "$path"
}

resolve_project_root() {
  if command -v git >/dev/null 2>&1; then
    local git_root
    git_root=$(git rev-parse --show-toplevel 2>/dev/null || true)
    if [[ -n "$git_root" ]]; then
      printf '%s' "$git_root"
      return 0
    fi
  fi
  pwd
}

is_abs_path() {
  [[ "$1" == /* ]]
}

normalize_project_paths() {
  PROJECT_ROOT=$(resolve_project_root)

  if [[ -n "$specs_dir" ]] && ! is_abs_path "$specs_dir"; then
    specs_dir="$PROJECT_ROOT/$specs_dir"
  fi
  if [[ -n "$plans_dir" ]] && ! is_abs_path "$plans_dir"; then
    plans_dir="$PROJECT_ROOT/$plans_dir"
  fi
  if [[ -n "$log_dir" ]] && ! is_abs_path "$log_dir"; then
    log_dir="$PROJECT_ROOT/$log_dir"
  fi
  if [[ -n "$prompt_file" ]] && ! is_abs_path "$prompt_file"; then
    prompt_file="$PROJECT_ROOT/$prompt_file"
  fi
}

resolve_existing_file() {
  local path="$1"
  local primary_base="${2:-}"
  local secondary_base="${3:-}"

  if [[ -f "$path" ]]; then
    printf '%s' "$path"
    return 0
  fi

  if [[ -n "$PROJECT_ROOT" && -f "$PROJECT_ROOT/$path" ]]; then
    printf '%s' "$PROJECT_ROOT/$path"
    return 0
  fi

  if [[ -n "$primary_base" ]]; then
    if [[ -f "$primary_base/$path" ]]; then
      printf '%s' "$primary_base/$path"
      return 0
    fi
    if [[ -f "$primary_base/$(basename "$path")" ]]; then
      printf '%s' "$primary_base/$(basename "$path")"
      return 0
    fi
  fi

  if [[ -n "$secondary_base" ]]; then
    if [[ -f "$secondary_base/$path" ]]; then
      printf '%s' "$secondary_base/$path"
      return 0
    fi
    if [[ -f "$secondary_base/$(basename "$path")" ]]; then
      printf '%s' "$secondary_base/$(basename "$path")"
      return 0
    fi
  fi

  return 1
}

load_config() {
  local project_root
  project_root=$(resolve_project_root)
  local project_path="$project_root/.loop/config"

  load_config_file "$project_path"

  if [[ -n "$config_path" ]]; then
    load_config_file "$config_path" "true"
  fi
}

count_plan_pending_tasks() {
  local plan="$1"
  awk '
    /^[[:space:]]*```/ {in_code = !in_code; next}
    in_code {next}
    /^[[:space:]]*- \[ \]\?/ {next}
    /^[[:space:]]*- \[~\]([[:space:]]|$)/ {pending++}
    /^[[:space:]]*- \[ \]([[:space:]]|$)/ {pending++}
    END {print pending + 0}
  ' "$plan"
}

count_plan_items_to_review() {
  local plan="$1"
  awk '
    /^[[:space:]]*```/ {in_code = !in_code; next}
    in_code {next}
    /^[[:space:]]*- \[[xX]\]([[:space:]]|$)/ {n++}
    END {print n + 0}
  ' "$plan"
}

find_early_flags() {
  local args=("$@")
  for ((i=0; i<${#args[@]}; i++)); do
    case "${args[$i]}" in
      --config)
        if (( i + 1 < ${#args[@]} )); then
          config_path="${args[$((i + 1))]}"
        fi
        ;;
      --config=*)
        config_path="${args[$i]#--config=}"
        ;;
      --init-config)
        init_config=true
        ;;
    esac
  done
}

init_config_file() {
  local project_root
  project_root=$(resolve_project_root)
  local target_path
  if [[ -n "$config_path" ]]; then
    target_path="$config_path"
  else
    target_path="$project_root/.loop/config"
  fi

  if [[ -e "$target_path" ]]; then
    echo "Error: Config already exists: $target_path" >&2
    return 1
  fi

  local target_dir
  target_dir=$(dirname "$target_path")
  if ! mkdir -p "$target_dir"; then
    echo "Error: Cannot create config directory: $target_dir" >&2
    return 1
  fi

  cat > "$target_path" <<EOF
specs_dir="$specs_dir"
plans_dir="$plans_dir"
log_dir="$log_dir"
model="$model"
iterations=$iterations
completion_mode="$completion_mode"
mode="$mode"
postmortem=$postmortem
summary_json=$summary_json
no_wait=$no_wait
no_gum=$no_gum
reviewer=$reviewer
verify_cmds="$verify_cmds"
verify_timeout_sec=$verify_timeout_sec
measure_cmd="$measure_cmd"
measure_timeout_sec=$measure_timeout_sec
claude_timeout_sec=$claude_timeout_sec
claude_retries=$claude_retries
claude_retry_backoff_sec=$claude_retry_backoff_sec
# prompt_file=""
# context_files="specs/README.md specs/planning/SPEC_AUTHORING.md"
EOF

  printf 'Created config at %s\n' "$target_path"
}

# -----------------------------------------------------------------------------
# Argument parsing
# -----------------------------------------------------------------------------
parse_args() {
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --iterations)
        iterations="$2"
        shift 2
        ;;
      --log-dir)
        log_dir="$2"
        shift 2
        ;;
      --model)
        model="$2"
        shift 2
        ;;
      --config)
        config_path="$2"
        shift 2
        ;;
      --config=*)
        config_path="${1#--config=}"
        shift
        ;;
      --prompt)
        prompt_file="$2"
        shift 2
        ;;
      --mode)
        mode="$2"
        shift 2
        ;;
      --verify-cmd)
        if [[ -z "$verify_cmds" ]]; then
          verify_cmds="$2"
        else
          verify_cmds+="|$2"
        fi
        shift 2
        ;;
      --verify-timeout-sec)
        verify_timeout_sec="$2"
        shift 2
        ;;
      --measure-cmd)
        measure_cmd="$2"
        shift 2
        ;;
      --measure-timeout-sec)
        measure_timeout_sec="$2"
        shift 2
        ;;
      --claude-timeout-sec)
        claude_timeout_sec="$2"
        shift 2
        ;;
      --claude-retries)
        claude_retries="$2"
        shift 2
        ;;
      --claude-retry-backoff-sec)
        claude_retry_backoff_sec="$2"
        shift 2
        ;;
      --init-config)
        init_config=true
        shift
        ;;
      --completion-mode)
        completion_mode="$2"
        shift 2
        ;;
      --postmortem)
        postmortem=true
        shift
        ;;
      --no-postmortem)
        postmortem=false
        shift
        ;;
      --no-gum)
        no_gum=true
        shift
        ;;
      --reviewer)
        reviewer=true
        shift
        ;;
      --no-reviewer)
        reviewer=false
        shift
        ;;
      --summary-json)
        summary_json=true
        shift
        ;;
      --no-wait)
        no_wait=true
        shift
        ;;
      -h|--help)
        usage
        exit 0
        ;;
      -*)
        echo "Unknown option: $1" >&2
        usage >&2
        exit 1
        ;;
      prompt)
        # Subcommand: show prompt
        show_prompt=true
        shift
        ;;
      *)
        # Positional arguments
        if [[ -z "$spec_path" ]]; then
          spec_path="$1"
        elif [[ -z "$plan_path" ]]; then
          plan_path="$1"
        else
          echo "Unexpected argument: $1" >&2
          usage >&2
          exit 1
        fi
        shift
        ;;
    esac
  done
}

# -----------------------------------------------------------------------------
# Validation
# -----------------------------------------------------------------------------
validate_inputs() {
  # Check if spec path is required
  if [[ -z "$spec_path" ]]; then
    # If gum is available and not disabled, use spec picker
    if [[ "$no_gum" == "true" ]] || ! check_gum; then
      echo "Error: spec-path is required when gum is unavailable or --no-gum is set" >&2
      list_known_specs >&2
      usage >&2
      exit 1
    else
      # Launch interactive spec picker
      if ! spec_picker; then
        echo "Error: No spec selected" >&2
        exit 1
      fi
      spec_path="$PICKED_SPEC_PATH"
      plan_path="$PICKED_PLAN_PATH"
    fi
  fi

  local resolved_spec
  resolved_spec=$(resolve_existing_file "$spec_path" "$specs_dir" || true)
  if [[ -n "$resolved_spec" ]]; then
    spec_path="$resolved_spec"
  fi

  # Derive plan path if not provided
  if [[ -z "$plan_path" ]]; then
    local spec_base
    spec_base=$(basename "$spec_path")
    local candidate_plan="$plans_dir/${spec_base%.md}-plan.md"
    if [[ "$mode" != "experiment" || -f "$candidate_plan" ]]; then
      plan_path="$candidate_plan"
    fi
  fi

  if [[ -n "$plan_path" ]]; then
    local resolved_plan
    resolved_plan=$(resolve_existing_file "$plan_path" "$plans_dir" || true)
    if [[ -n "$resolved_plan" ]]; then
      plan_path="$resolved_plan"
    fi
  fi

  # Validate spec exists
  if [[ ! -f "$spec_path" ]]; then
    echo "Error: Spec not found: $spec_path" >&2
    exit 1
  fi

  # Validate plan exists unless experiment mode
  if [[ "$mode" != "experiment" ]]; then
    if [[ ! -f "$plan_path" ]]; then
      echo "Error: Plan not found: $plan_path" >&2
      echo "Hint: Create plan at $plan_path" >&2
      exit 1
    fi
  else
    if [[ -n "$plan_path" && ! -f "$plan_path" ]]; then
      plan_path=""
    fi
  fi

  # Validate iterations is a number
  if ! [[ "$iterations" =~ ^[0-9]+$ ]]; then
    echo "Error: --iterations must be a positive integer" >&2
    exit 1
  fi

  if [[ "$completion_mode" != "exact" && "$completion_mode" != "trailing" ]]; then
    echo "Error: --completion-mode must be 'exact' or 'trailing'" >&2
    exit 1
  fi

  if [[ "$mode" != "plan" && "$mode" != "experiment" ]]; then
    echo "Error: --mode must be 'plan' or 'experiment'" >&2
    exit 1
  fi

  if ! [[ "$verify_timeout_sec" =~ ^[0-9]+$ ]]; then
    echo "Error: verify_timeout_sec/--verify-timeout-sec must be a non-negative integer" >&2
    exit 1
  fi
  if ! [[ "$measure_timeout_sec" =~ ^[0-9]+$ ]]; then
    echo "Error: measure_timeout_sec/--measure-timeout-sec must be a non-negative integer" >&2
    exit 1
  fi
  if ! [[ "$claude_timeout_sec" =~ ^[0-9]+$ ]]; then
    echo "Error: claude_timeout_sec/--claude-timeout-sec must be a non-negative integer" >&2
    exit 1
  fi
  if ! [[ "$claude_retries" =~ ^[0-9]+$ ]]; then
    echo "Error: claude_retries/--claude-retries must be a non-negative integer" >&2
    exit 1
  fi
  if ! [[ "$claude_retry_backoff_sec" =~ ^[0-9]+$ ]]; then
    echo "Error: claude_retry_backoff_sec/--claude-retry-backoff-sec must be a non-negative integer" >&2
    exit 1
  fi

  if [[ "$mode" == "experiment" && -z "$measure_cmd" ]]; then
    echo "Error: measure_cmd/--measure-cmd is required in experiment mode" >&2
    exit 1
  fi
}

# -----------------------------------------------------------------------------
# Prompt loading
# -----------------------------------------------------------------------------
ensure_learnings_file() {
  local learnings_path="$specs_dir/LEARNINGS.md"
  if [[ ! -f "$learnings_path" ]]; then
    mkdir -p "$specs_dir"
    cat > "$learnings_path" <<'EOF'
# Learnings

Repo-wide patterns and lessons learned from implementation reviews.
Reviewers add entries here when they find patterns that apply beyond a single task.
Periodically curate this into proper codebase rules (CLAUDE.md, lint configs, etc).

---

EOF
    ui_log "INFO" "Created learnings file: $learnings_path"
  fi
  printf '%s' "$learnings_path"
}

build_context_refs() {
  local refs="@SPEC_PATH"

  if [[ -n "$plan_path" && -f "$plan_path" ]]; then
    refs="$refs @PLAN_PATH"
  fi

  if [[ -n "${RUNNER_NOTES_PATH:-}" ]]; then
    refs="$refs @$RUNNER_NOTES_PATH"
  fi

  if [[ -n "${EXPERIMENT_LOG_PATH:-}" ]]; then
    refs="$refs @$EXPERIMENT_LOG_PATH"
  fi

  # Add learnings file if it exists
  local learnings_path="$specs_dir/LEARNINGS.md"
  if [[ -f "$learnings_path" ]]; then
    refs="$refs @$learnings_path"
  fi

  if [[ -n "$context_files" ]]; then
    for file in $context_files; do
      if is_abs_path "$file"; then
        refs="$refs @$file"
      else
        refs="$refs @$PROJECT_ROOT/$file"
      fi
    done
  fi
  printf '%s' "$refs"
}

split_verify_cmds() {
  local raw="$1"
  local -n out_arr=$2

  out_arr=()
  if [[ -z "$raw" ]]; then
    return 0
  fi

  local IFS='|'
  read -r -a out_arr <<< "$raw"

  # Trim empty entries
  local -a cleaned=()
  for cmd in "${out_arr[@]}"; do
    cmd="${cmd#${cmd%%[!$' \t\n\r']*}}"
    cmd="${cmd%${cmd##*[!$' \t\n\r']}}"
    [[ -n "$cmd" ]] && cleaned+=("$cmd")
  done
  out_arr=("${cleaned[@]}")
}

write_runner_notes() {
  local text="$1"
  if [[ -z "${RUNNER_NOTES_PATH:-}" ]]; then
    return 0
  fi
  printf '%s\n' "$text" > "$RUNNER_NOTES_PATH"
}

clear_runner_notes() {
  write_runner_notes ""
}

have_timeout_cmd() {
  command -v timeout >/dev/null 2>&1
}

run_verify_cmd() {
  local cmd="$1"
  local out_path="$2"
  local exit_code=0

  if ((verify_timeout_sec > 0)) && have_timeout_cmd; then
    timeout "${verify_timeout_sec}s" bash -lc "$cmd" >"$out_path" 2>&1 || exit_code=$?
  else
    if ((verify_timeout_sec > 0)) && ! have_timeout_cmd; then
      ui_log "WARN" "verify_timeout_sec set but 'timeout' not found; running without timeout"
    fi
    bash -lc "$cmd" >"$out_path" 2>&1 || exit_code=$?
  fi

  return "$exit_code"
}

get_git_head() {
  git rev-parse HEAD 2>/dev/null || true
}

is_git_clean() {
  if ! git diff --quiet --ignore-submodules --; then
    return 1
  fi
  if ! git diff --cached --quiet --ignore-submodules --; then
    return 1
  fi
  return 0
}

should_run_verification() {
  if ! git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
    return 0
  fi

  local head
  head=$(get_git_head)
  if [[ -z "$head" ]]; then
    return 0
  fi

  if [[ -n "$LAST_VERIFIED_HEAD" && "$head" == "$LAST_VERIFIED_HEAD" ]]; then
    if is_git_clean; then
      return 1
    fi
  fi

  return 0
}

maybe_run_verification_phase() {
  local iter_label="$1"

  if [[ -z "$verify_cmds" ]]; then
    return 0
  fi

  if ! should_run_verification; then
    ui_log "INFO" "Verification skipped (no changes since last success)"
    report_event "VERIFY_SKIP" "$iter_label" "" "" "" "" "" "reason=no_changes"
    return 0
  fi

  run_verification_phase "$iter_label"
}

run_verification_phase() {
  local iter_label="$1"

  local -a cmds=()
  split_verify_cmds "$verify_cmds" cmds
  if [[ ${#cmds[@]} -eq 0 ]]; then
    return 0
  fi

  local iter_slug
  iter_slug=$(format_iteration_slug "$iter_label")

  local verify_dir="$RUN_DIR/verify"
  mkdir -p "$verify_dir"

  local overall_ok=true
  local failed_cmd=""
  local failed_code=0
  local failed_log=""

  ui_log "INFO" "Verification starting (${#cmds[@]} cmd(s))"
  report_event "VERIFY_START" "$iter_label" "" "" "" "" "" "cmds=${#cmds[@]}"

  for ((idx=0; idx<${#cmds[@]}; idx++)); do
    local cmd="${cmds[$idx]}"
    local cmd_num=$((idx + 1))
    local out_path="$verify_dir/verify-$iter_slug-$cmd_num.log"

    ui_log "INFO" "Verify [$cmd_num/${#cmds[@]}]: $cmd"
    report_event "VERIFY_CMD_START" "$iter_label" "" "" "" "" "$out_path" "cmd=$cmd"

    local start_ms
    start_ms=$(get_epoch_ms)
    local exit_code=0
    run_verify_cmd "$cmd" "$out_path" || exit_code=$?
    local end_ms
    end_ms=$(get_epoch_ms)
    local dur_ms=$((end_ms - start_ms))

    local out_bytes out_lines
    out_bytes=$(wc -c < "$out_path" | tr -d ' ')
    out_lines=$(wc -l < "$out_path" | tr -d ' ')

    report_event "VERIFY_CMD_END" "$iter_label" "$dur_ms" "$exit_code" "$out_bytes" "$out_lines" "$out_path" "cmd=$cmd"

    if ((exit_code != 0)); then
      overall_ok=false
      failed_cmd="$cmd"
      failed_code=$exit_code
      failed_log="$out_path"
      break
    fi
  done

  if [[ "$overall_ok" == "true" ]]; then
    ui_log "INFO" "Verification passed"
    report_event "VERIFY_END" "$iter_label" "" "0" "" "" "" "status=ok"
    clear_runner_notes

    if git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
      LAST_VERIFIED_HEAD=$(get_git_head)
    fi
    return 0
  fi

  ui_log "ERROR" "Verification failed (exit=$failed_code): $failed_cmd"
  report_event "VERIFY_END" "$iter_label" "" "$failed_code" "" "" "$failed_log" "status=failed"

  local failure_context="$verify_dir/verify-$iter_slug-failure.txt"
  {
    printf 'Verification failed. Fix this before proceeding.\n\n'
    printf 'Command: %s\n' "$failed_cmd"
    printf 'Exit code: %s\n\n' "$failed_code"
    printf 'Log: %s\n\n' "$failed_log"
    printf 'Last 120 lines:\n'
    tail -n 120 "$failed_log" || true
  } > "$failure_context"

  write_runner_notes "Runner detected failing verification.

Read this failure context carefully and fix it before doing new plan work:

$(cat "$failure_context")

Runner instructions (highest priority):
- Fix the verification failure(s) from the most recent commit.
- Do NOT mark additional plan items as [x] while fixing verification.
- After fixing, rerun the verification commands and commit the fix.
"

  return 1
}

run_measure_cmd() {
  local cmd="$1"
  local out_path="$2"
  local exit_code=0

  if ((measure_timeout_sec > 0)) && have_timeout_cmd; then
    timeout "${measure_timeout_sec}s" bash -lc "$cmd" >"$out_path" 2>&1 || exit_code=$?
  else
    if ((measure_timeout_sec > 0)) && ! have_timeout_cmd; then
      ui_log "WARN" "measure_timeout_sec set but 'timeout' not found; running without timeout"
    fi
    bash -lc "$cmd" >"$out_path" 2>&1 || exit_code=$?
  fi

  return "$exit_code"
}

run_measure_phase() {
  local iter_label="$1"

  if [[ -z "$measure_cmd" ]]; then
    return 0
  fi

  local iter_slug
  iter_slug=$(format_iteration_slug "$iter_label")

  local metrics_dir="${EXPERIMENT_METRICS_DIR:-$RUN_DIR/metrics}"
  mkdir -p "$metrics_dir"

  local metrics_path="$metrics_dir/iter-$iter_slug.json"
  local log_path="$metrics_dir/iter-$iter_slug.log"

  ui_log "INFO" "Measurement starting"
  report_event "MEASURE_START" "$iter_label" "" "" "" "" "$log_path" "cmd=$measure_cmd"

  local start_ms
  start_ms=$(get_epoch_ms)
  local exit_code=0

  LOOP_METRICS_OUT="$metrics_path" \
    LOOP_ITERATION="$iter_label" \
    LOOP_RUN_DIR="$RUN_DIR" \
    LOOP_SPEC_PATH="$spec_path" \
    run_measure_cmd "$measure_cmd" "$log_path" || exit_code=$?

  local end_ms
  end_ms=$(get_epoch_ms)
  local dur_ms=$((end_ms - start_ms))

  local out_bytes out_lines
  out_bytes=$(wc -c < "$log_path" | tr -d ' ')
  out_lines=$(wc -l < "$log_path" | tr -d ' ')

  if ((exit_code == 0)); then
    local message="metrics=$metrics_path"
    if [[ ! -f "$metrics_path" ]]; then
      ui_log "WARN" "Measurement did not produce metrics file: $metrics_path"
      message+=" missing_metrics=true"
    fi
    report_event "MEASURE_END" "$iter_label" "$dur_ms" "$exit_code" "$out_bytes" "$out_lines" "$log_path" "$message"
    return 0
  fi

  ui_log "ERROR" "Measurement failed (exit=$exit_code)"
  report_event "MEASURE_END" "$iter_label" "$dur_ms" "$exit_code" "$out_bytes" "$out_lines" "$log_path" "failed"
  return "$exit_code"
}

write_experiment_summary() {
  local iter_label="$1"
  local commit_hash="$2"
  local agent_summary="$3"
  local metrics_path="$4"
  local measure_log="$5"
  local verify_status="$6"
  local measure_status="$7"

  local iter_slug
  iter_slug=$(format_iteration_slug "$iter_label")

  local summaries_dir="${EXPERIMENT_SUMMARY_DIR:-$RUN_DIR/summaries}"
  mkdir -p "$summaries_dir"

  local summary_path="$summaries_dir/iter-$iter_slug.md"

  {
    printf '# Iteration %s\n\n' "$iter_label"
    if [[ -n "$commit_hash" ]]; then
      printf -- '- Commit: %s\n' "$commit_hash"
    else
      printf -- '- Commit: (none)\n'
    fi
    if [[ -n "$agent_summary" ]]; then
      printf -- '- Agent: %s\n' "$agent_summary"
    fi
    if [[ -n "$verify_status" ]]; then
      printf -- '- Verification: %s\n' "$verify_status"
    fi
    if [[ -n "$measure_status" ]]; then
      printf -- '- Measurement: %s\n' "$measure_status"
    fi
    if [[ -n "$metrics_path" ]]; then
      printf -- '- Metrics: %s\n' "$metrics_path"
    fi
    if [[ -n "$measure_log" ]]; then
      printf -- '- Measure log: %s\n' "$measure_log"
    fi

    if [[ -n "$commit_hash" ]] && git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
      printf '\n## Diff Summary\n\n'
      git show -1 --stat "$commit_hash" || true
    fi
  } > "$summary_path"

  report_event "ITERATION_SUMMARY" "$iter_label" "" "" "" "" "$summary_path" ""
  printf '%s' "$summary_path"
}

append_experiment_log() {
  local iter_label="$1"
  local commit_hash="$2"
  local metrics_path="$3"
  local summary_path="$4"

  local log_path="${EXPERIMENT_LOG_PATH:-}"
  if [[ -z "$log_path" ]]; then
    return 0
  fi

  {
    printf '## Iteration %s\n' "$iter_label"
    if [[ -n "$commit_hash" ]]; then
      printf -- '- Commit: %s\n' "$commit_hash"
    fi
    if [[ -n "$metrics_path" ]]; then
      printf -- '- Metrics: %s\n' "$metrics_path"
    fi
    if [[ -n "$summary_path" ]]; then
      printf -- '- Summary: %s\n' "$summary_path"
    fi
    printf '\n'
  } >> "$log_path"
}

init_experiment_artifacts() {
  EXPERIMENT_LOG_PATH="$RUN_DIR/experiment-log.md"
  EXPERIMENT_SUMMARY_DIR="$RUN_DIR/summaries"
  EXPERIMENT_METRICS_DIR="$RUN_DIR/metrics"

  mkdir -p "$EXPERIMENT_SUMMARY_DIR" "$EXPERIMENT_METRICS_DIR"

  if [[ ! -f "$EXPERIMENT_LOG_PATH" ]]; then
    {
      printf '# Experiment Log\n\n'
      printf -- '- Run ID: %s\n' "$RUN_ID"
      printf -- '- Spec: %s\n' "$spec_path"
      if [[ -n "$plan_path" ]]; then
        printf -- '- Plan: %s\n' "$plan_path"
      fi
      printf '\n'
    } > "$EXPERIMENT_LOG_PATH"
  fi
}

run_claude_with_retries() {
  local iter_label="$1"
  local prompt="$2"
  local model="$3"
  local -n out_ref=$4

  local attempt=1
  local max_attempts=$((claude_retries + 1))
  local exit_code=0

  while ((attempt <= max_attempts)); do
    local this_label="$iter_label"
    if ((max_attempts > 1)); then
      this_label="${iter_label}A${attempt}"
    fi

    out_ref=""
    run_claude_iteration "$this_label" "$prompt" "$model" out_ref || exit_code=$?
    if ((exit_code == 0)); then
      return 0
    fi

    if ((attempt == max_attempts)); then
      return "$exit_code"
    fi

    ui_log "WARN" "Claude failed (exit=$exit_code) - retrying ($attempt/$max_attempts)"
    report_event "CLAUDE_RETRY" "$this_label" "" "$exit_code" "" "" "$ITER_LOG_PATH" "attempt=$attempt"
    sleep "$claude_retry_backoff_sec"
    attempt=$((attempt + 1))
  done

  return "$exit_code"
}

run_experiment_loop() {
  local prompt="$1"

  for ((i=1; i<=iterations; i++)); do
    local result=""
    local claude_exit=0

    local head_before=""
    if git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
      head_before=$(get_git_head)
    fi

    run_claude_with_retries "$i" "$prompt" "$model" result || claude_exit=$?

    printf '%s\n' "$result"

    if ((claude_exit != 0)); then
      show_run_summary "claude_failed" "$claude_exit"
      [[ "$summary_json" == "true" ]] && write_summary_json "claude_failed" "$claude_exit"
      run_postmortem "claude_failed" || true
      show_completion_screen "$no_wait"
      exit "$claude_exit"
    fi

    local head_after=""
    if git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
      head_after=$(get_git_head)
    fi

    local commit_hash=""
    if [[ -n "$head_before" && -n "$head_after" && "$head_before" != "$head_after" ]]; then
      commit_hash="$head_after"
    fi

    local agent_summary=""
    agent_summary=$(printf '%s\n' "$result" | awk 'NF {print; exit}')

    local verify_status="not_configured"
    if [[ -n "$verify_cmds" ]]; then
      local should_verify=true
      if ! should_run_verification; then
        should_verify=false
      fi

      local verify_exit=0
      maybe_run_verification_phase "$i" || verify_exit=$?

      if [[ "$should_verify" == "false" ]]; then
        verify_status="skipped"
      elif ((verify_exit == 0)); then
        verify_status="passed"
      else
        verify_status="failed"
      fi
    fi

    local measure_status="not_configured"
    local metrics_path=""
    local measure_log=""
    local iter_slug
    iter_slug=$(format_iteration_slug "$i")
    if [[ -n "$measure_cmd" ]]; then
      metrics_path="$EXPERIMENT_METRICS_DIR/iter-$iter_slug.json"
      measure_log="$EXPERIMENT_METRICS_DIR/iter-$iter_slug.log"

      if [[ "$verify_status" == "failed" ]]; then
        measure_status="skipped (verify failed)"
      else
        local measure_exit=0
        run_measure_phase "$i" || measure_exit=$?
        if ((measure_exit == 0)); then
          if [[ -f "$metrics_path" ]]; then
            measure_status="ok"
          else
            measure_status="missing_output"
          fi
        else
          measure_status="failed"
        fi
      fi
    fi

    local summary_path
    summary_path=$(write_experiment_summary "$i" "$commit_hash" "$agent_summary" "$metrics_path" "$measure_log" "$verify_status" "$measure_status")
    append_experiment_log "$i" "$commit_hash" "$metrics_path" "$summary_path"
  done

  show_run_summary "iterations_exhausted" "0"
  [[ "$summary_json" == "true" ]] && write_summary_json "iterations_exhausted" "0"
  run_postmortem "iterations_exhausted" || true
  show_completion_screen "$no_wait"
}

load_prompt() {
  local spec="$1"
  local plan="$2"
  local project_root
  project_root=$(resolve_project_root)

  # Check for custom prompt file
  local custom_prompt=""
  if [[ -n "$prompt_file" && -f "$prompt_file" ]]; then
    custom_prompt="$prompt_file"
  elif [[ -f "$project_root/.loop/prompt.txt" ]]; then
    custom_prompt="$project_root/.loop/prompt.txt"
  fi

  local context_refs
  context_refs=$(build_context_refs)

  local prompt
  if [[ -n "$custom_prompt" ]]; then
    prompt=$(cat "$custom_prompt")
  else
    if [[ "$mode" == "experiment" ]]; then
      prompt="$context_refs

You are an experiment agent. Your goal is to improve the outcome described in the spec.

Read the experiment log (if present) to avoid repeating past attempts.

Task:
1. Identify one focused change to improve the target outcome.
2. Implement the change, keeping the scope tight and reversible.
3. Run any local checks that help validate the change (if applicable).
4. Make exactly one git commit for your changes using \`gritty commit --accept\`.

Notes:
- The runner will execute verification and measurement commands after your iteration.
- Do not output the completion token in experiment mode.

Response format (strict):
- ONE sentence: \"Experimented: <short summary>.\"
- If no changes were made: \"No change this iteration.\"

Constraints:
- Do not modify files under \`reference/\`.
- Keep changes minimal and directly related to the experiment goal."
    else
      local completion_note
      if [[ "$completion_mode" == "exact" ]]; then
        completion_note="The runner detects completion only if your entire output is exactly <promise>COMPLETE</promise>."
      else
        completion_note="The runner detects completion when the last non-empty line is exactly <promise>COMPLETE</promise>."
      fi

      # Default prompt - use quoted heredoc to prevent interpretation of < and >
      prompt="$context_refs

You are an implementation agent. Read the spec and the plan.

IMPORTANT: Before starting work, check:
1. The LEARNINGS.md file for repo-wide patterns and common mistakes
2. The ## Learnings section at the bottom of the plan for task-specific corrections
Avoid repeating past mistakes - these learnings exist because previous implementations got it wrong.

Task:
1. Choose ONE unchecked task from the plan with the highest priority (not necessarily first).
2. Implement only that task (single feature). Avoid unrelated changes.
3. Run verification relevant to that task. If the plan lists a verification checklist, run what
   applies. If you cannot run a step, add a note to the plan's \`## Notes\` or \`## Blockers Discovered\` section.
4. Update the plan checklist: mark only the task(s) you completed with [x]. Leave others untouched.
5. Make exactly one git commit for your changes using \`gritty commit --accept\`.
6. If a task is blocked by a production bug or missing test infrastructure, mark it \`[~]\` and add it to
   the plan's \`## Blockers Discovered\` section. Do not mark it \`[x]\`.
7. If (and only if) all \`[ ]\` and \`[~]\` tasks in the plan are complete (ignore \`[ ]?\` manual QA items), respond with:
<promise>COMPLETE</promise>

Spec alignment guardrails (must follow):
- Before coding, identify the exact spec section(s) you are implementing and list the required
  behavior, constraints, and any data shapes.
- If the spec defines a schema/event payload/API contract, match it exactly (field names,
  nesting, nullability, ordering). Keep types in sync.
- Do not use placeholder values for required behavior. Implement the real behavior or leave the
  task unchecked.
- If any spec detail is ambiguous, do not guess. Choose the safest minimal interpretation,
  document the assumption in your response, and limit changes to what is unambiguous.

Response format (strict):
- ALL \`[ ]\` tasks complete: output \`<promise>COMPLETE</promise>\`.
  If the runner requires exact output, print only the token; otherwise ensure it's the final non-empty line.
- Tasks remain: ONE sentence only.
  - If you completed a task: \"Completed [task]. [N] tasks remain.\"
  - If you marked a task \`[~]\`: \"Blocked [task]. [N] tasks remain.\"
  (N = unchecked \`[ ]\` + \`[~]\` items only)
  Multi-sentence output wastes context and delays completion.

Constraints:
- Do not modify files under \`reference/\`.
- Do not work on more than one plan item.
- If no changes were made, do not commit.

$completion_note"
    fi
  fi

  # Substitute placeholders
  prompt=${prompt//SPEC_PATH/$spec}
  prompt=${prompt//PLAN_PATH/$plan}

  printf '%s' "$prompt"
}

preflight_checks() {
  if ! command -v claude >/dev/null 2>&1; then
    echo "Error: claude CLI not found in PATH" >&2
    echo "Install: https://docs.anthropic.com/en/docs/claude-code" >&2
    exit 1
  fi

  local project_root
  project_root=$(resolve_project_root)
  local has_custom_prompt=false

  if [[ -n "$prompt_file" && -f "$prompt_file" ]]; then
    has_custom_prompt=true
  elif [[ -f "$project_root/.loop/prompt.txt" ]]; then
    has_custom_prompt=true
  fi

  if [[ "$has_custom_prompt" == "false" ]] && ! command -v gritty >/dev/null 2>&1; then
    echo "Error: gritty not found; default prompt requires it to commit" >&2
    echo "Install: https://github.com/ohitslaurence/gritty" >&2
    echo "Or create $project_root/.loop/prompt.txt and remove gritty usage." >&2
    exit 1
  fi

  if ! git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
    ui_log "WARN" "Not inside a git repo - commits/review may fail"
  fi
}

# -----------------------------------------------------------------------------
# Reviewer prompt
# -----------------------------------------------------------------------------
load_reviewer_prompt() {
  local spec="$1"
  local plan="$2"

  local prompt="@$spec @$plan

You are a senior staff engineer reviewing implementation work.

## Your Role

Review the most recent implementation against:

1. **Spec compliance** - Does it match requirements exactly? Check data shapes, field names,
   serialization, event ordering - all the details the spec defines.

2. **Codebase consistency** - Does it follow the existing patterns in this repo?
   - If the repo uses Effect-TS, code should use Effect, not plain Promises
   - If the repo uses a Result type (neverthrow, fp-ts, etc.), errors should use that pattern
   - Match existing naming conventions, file organization, module patterns
   - Use the same libraries/utilities already in use, don't introduce alternatives

3. **Code quality** - Is it readable, maintainable, appropriately abstracted?
   - No over-engineering, but no under-engineering either
   - Clear intent, good names, appropriate comments (not excessive)

4. **Best practices** - Error handling, edge cases, security considerations?
   - Validate at boundaries, trust internal code
   - Handle failure modes that can actually happen
   - No obvious security issues (injection, XSS, etc.)

5. **Performance** - Any obvious inefficiencies?
   - Unnecessary allocations, N+1 queries, blocking in async code
   - Only flag clear issues, not premature optimization

## Task

1. Find the first task marked \`[x]\` (implemented but not yet \`[R]\` reviewed).
2. Read the commit for that task: \`git log -1 --stat\` and \`git show HEAD\`.
3. Check LEARNINGS.md and plan's ## Learnings section for patterns to watch for.
4. Review holistically against the criteria above.
5. Decision:
   - **Acceptable**: Mark \`[x]\` → \`[R]\` in the plan. Done.
   - **Needs correction**: Fix the issues, commit with \`gritty commit --accept\`,
     add a note to ## Learnings explaining what was wrong, then mark \`[R]\`.
   - **Needs rethinking**: If fundamentally wrong approach, mark back to \`[ ]\`,
     add detailed learning note explaining the correct approach.

## Learnings

Two places for learnings - use the right one:

**Plan ## Learnings section** (task-specific):
- Corrections specific to this spec/task
- Format: \`- <Task name>: <Brief note>\`
- Example: \`- Session storage: Was using in-memory Map, spec requires Redis (§3.2)\`

**specs/LEARNINGS.md** (repo-wide):
- Patterns that apply across all specs in this codebase
- Things future agents should know about this repo's conventions
- Format: \`## <Category>\` then bullet points
- Examples:
  - \"This repo uses Effect-TS - never use plain Promises or try/catch\"
  - \"All API responses use the Result type from src/lib/result.ts\"
  - \"Database access goes through repositories in src/repos/, never direct queries\"

If task is acceptable with no issues, don't add noise to either location.

## Response Format

- If \`[x]\` tasks remain to review: one sentence only - \"Reviewed [task]. [N] awaiting review.\"
- If all implemented tasks are now \`[R]\`:
  - If any \`[ ]\` tasks remain: \"Review complete. Returning to implementation.\"
  - If all tasks are \`[R]\`: exactly \`<promise>COMPLETE</promise>\`

## Constraints

- Review ONE task per iteration
- Do not implement new \`[ ]\` tasks - only review existing \`[x]\` items
- Do not modify files under \`reference/\`
- Prefer minimal targeted fixes over rewrites
- Match the codebase style, not your preferred style"

  printf '%s' "$prompt"
}

# -----------------------------------------------------------------------------
# Reviewer loop
# -----------------------------------------------------------------------------
has_items_to_review() {
  local plan="$1"
  # Check if there are [x] items that aren't [R] (reviewed)
  # [x] = implemented but not reviewed, [R] = reviewed
  grep -qE '^\s*-\s*\[x\]' "$plan" 2>/dev/null
}

run_reviewer_phase() {
  local spec="$1"
  local plan="$2"
  local model="$3"
  local impl_iteration="$4"
  local max_review_iters=10

  if ! has_items_to_review "$plan"; then
    return 0
  fi

  local reviewer_prompt
  reviewer_prompt=$(load_reviewer_prompt "$spec" "$plan")

  for ((r=1; r<=max_review_iters; r++)); do
    local review_iter="${impl_iteration}R${r}"
    local result=""
    local review_exit=0

    run_claude_with_retries "$review_iter" "$reviewer_prompt" "$model" result || review_exit=$?

    # Trim and check for completion signals
    local trimmed_result="$result"
    trimmed_result="${trimmed_result#"${trimmed_result%%[!$'\t\n\r ']*}"}"
    trimmed_result="${trimmed_result%"${trimmed_result##*[!$'\t\n\r ']}"}"

    printf '%s\n' "$result"

    # Reviewer signals return to implementation
    if [[ "$result" == *"Returning to implementation"* ]]; then
      ui_log "INFO" "Review phase complete, returning to implementation"
      return 0
    fi

    # All tasks reviewed and complete
    if [[ "$trimmed_result" == "<promise>COMPLETE</promise>" ]]; then
      ui_log "INFO" "Reviewer signaled all tasks complete"
      return 42  # Special exit code to signal full completion
    fi

    # Check if no more items to review
    if ! has_items_to_review "$plan"; then
      ui_log "INFO" "Review phase complete (no [x] items remaining)"
      return 0
    fi

    # Non-zero exit from claude
    if ((review_exit != 0)); then
      ui_log "WARN" "Reviewer iteration failed with exit $review_exit"
      return 0  # Continue to next implementation, don't block
    fi
  done

  ui_log "WARN" "Review loop exhausted after $max_review_iters iterations"
  return 0
}

# -----------------------------------------------------------------------------
# Postmortem automation
# -----------------------------------------------------------------------------
run_postmortem() {
  local reason="$1"

  if [[ "$postmortem" != "true" ]]; then
    return 0
  fi

  if ! command -v claude >/dev/null 2>&1; then
    ui_log "WARN" "Postmortem skipped: claude CLI not found"
    return 0
  fi

  local analyze_script="$LIB_DIR/../bin/loop-analyze"
  if [[ ! -x "$analyze_script" ]]; then
    analyze_script="$SCRIPT_DIR/loop-analyze"
  fi

  if [[ ! -x "$analyze_script" ]]; then
    ui_log "WARN" "Postmortem skipped: loop-analyze not found"
    return 0
  fi

  ui_header "Postmortem"
  ui_log "INFO" "Postmortem analysis starting ($reason)"
  report_event "POSTMORTEM_START" "" "" "" "" "" "" "reason=$reason"

  local -a analyze_args=("$RUN_ID" --log-dir "$log_dir" --run --model "$model")
  if [[ "$mode" == "experiment" ]]; then
    analyze_args+=(--experiment)
  fi

  if ! "$analyze_script" "${analyze_args[@]}"; then
    ui_log "WARN" "Postmortem analysis failed"
    report_event "POSTMORTEM_END" "" "" "" "" "" "" "status=failed"
    return 1
  fi

  ui_log "INFO" "Postmortem analysis complete"
  report_event "POSTMORTEM_END" "" "" "" "" "" "" "status=ok"
  return 0
}


# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------
main() {
  find_early_flags "$@"
  if [[ "$init_config" == "true" ]]; then
    init_config_file
    exit $?
  fi

  load_config
  parse_args "$@"
  normalize_project_paths
  validate_inputs

  PLAN_PATH="$plan_path"

  # Handle 'prompt' subcommand - just show prompt and exit
  if [[ "$show_prompt" == "true" ]]; then
    load_prompt "$spec_path" "$plan_path"
    printf '\n'
    exit 0
  fi

  preflight_checks

  # Initialize UI and logging
  if ! init_ui "$log_dir" "$no_gum"; then
    exit 1
  fi

  if [[ "$mode" == "experiment" ]]; then
    if [[ "$reviewer" == "true" ]]; then
      reviewer=false
      ui_log "INFO" "Reviewer disabled in experiment mode"
    fi
  fi

  # Ensure we operate from project root for relative verify commands.
  if [[ -n "$PROJECT_ROOT" ]]; then
    cd "$PROJECT_ROOT"
  fi

  # Configure UI-layer timeout handling.
  CLAUDE_TIMEOUT_SEC=$claude_timeout_sec

  refresh_plan_progress

  # Set up signal handlers for clean exit
  setup_signal_traps

  # Show run header
  show_run_header "$spec_path" "$plan_path" "$iterations" "$model" "$mode"

  # Ensure learnings file exists
  ensure_learnings_file >/dev/null

  # Create runner notes file (included in prompt via @ reference)
  RUNNER_NOTES_PATH="$RUN_DIR/runner-notes.txt"
  clear_runner_notes

  if [[ "$mode" == "experiment" ]]; then
    init_experiment_artifacts
  fi

  # Build prompt
  local prompt
  prompt=$(load_prompt "$spec_path" "$plan_path")

  write_prompt_snapshot "$prompt"

  if [[ "$mode" == "experiment" ]]; then
    run_experiment_loop "$prompt"
    return
  fi

  # Run loop
  for ((i=1; i<=iterations; i++)); do
    # Drain review backlog first (keeps quality tight and avoids "done but unreviewed" loops).
    if [[ "$reviewer" == "true" ]]; then
      local reviewer_exit=0
      local head_before=""
      local head_after=""
      if git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
        head_before=$(git rev-parse HEAD 2>/dev/null || true)
      fi

      run_reviewer_phase "$spec_path" "$plan_path" "$model" "$i" || reviewer_exit=$?

      if git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
        head_after=$(git rev-parse HEAD 2>/dev/null || true)
      fi

      if [[ -n "$head_before" && -n "$head_after" && "$head_before" != "$head_after" ]]; then
        maybe_run_verification_phase "${i}R" || true
      fi

      if ((reviewer_exit == 42)); then
        local remaining
        remaining=$(count_plan_pending_tasks "$plan_path")
        if ((remaining > 0)); then
          ui_log "WARN" "Reviewer signaled COMPLETE but plan still has pending tasks ($remaining)"
          report_event "WARN_REVIEWER_COMPLETE_WITH_PENDING" "$i" "" "" "" "" "$ITER_LOG_PATH" "pending=$remaining"
          continue
        fi

        if [[ -n "$verify_cmds" ]]; then
          if ! maybe_run_verification_phase "${i}FINAL"; then
            ui_log "WARN" "Final verification failed; continuing"
            continue
          fi
        fi

        record_completion "$i" "reviewer"
        show_run_summary "complete_reviewer" "0"
        [[ "$summary_json" == "true" ]] && write_summary_json "complete_reviewer" "0"
        run_postmortem "complete_reviewer" || true
        show_completion_screen "$no_wait"
        printf '<promise>COMPLETE</promise>\n'
        exit 0
      fi
    fi

    # If the plan is already complete, finish without spending another implementation iteration.
    local pending_tasks
    pending_tasks=$(count_plan_pending_tasks "$plan_path")
    local pending_review
    pending_review=$(count_plan_items_to_review "$plan_path")
    if ((pending_tasks == 0)); then
      if [[ "$reviewer" != "true" ]] || ((pending_review == 0)); then
        # Require verification to be green before declaring success.
        if [[ -n "$verify_cmds" ]]; then
          if ! maybe_run_verification_phase "${i}FINAL"; then
            ui_log "WARN" "Final verification failed; continuing"
            continue
          fi
        fi

        record_completion "$i" "plan"
        show_run_summary "complete_plan" "0"
        [[ "$summary_json" == "true" ]] && write_summary_json "complete_plan" "0"
        run_postmortem "complete_plan" || true
        show_completion_screen "$no_wait"
        printf '<promise>COMPLETE</promise>\n'
        exit 0
      fi

      # Plan is done but review backlog remains; avoid wasting an implementation iteration.
      ui_log "INFO" "Plan complete; continuing review-only ($pending_review items)"
      continue
    fi

    local result=""
    local claude_exit=0

    # Run claude with spinner and per-iteration logging
    run_claude_with_retries "$i" "$prompt" "$model" result || claude_exit=$?

    # Trim whitespace for comparison
    local trimmed_result="$result"
    trimmed_result="${trimmed_result#"${trimmed_result%%[!$'\t\n\r ']*}"}"
    trimmed_result="${trimmed_result%"${trimmed_result##*[!$'\t\n\r ']}"}"

    local last_nonempty_line
    last_nonempty_line=$(printf '%s\n' "$result" | awk 'NF {line=$0} END {print line}')
    local trimmed_last_line="$last_nonempty_line"
    trimmed_last_line="${trimmed_last_line#"${trimmed_last_line%%[!$'\t\n\r ']*}"}"
    trimmed_last_line="${trimmed_last_line%"${trimmed_last_line##*[!$'\t\n\r ']}"}"

    local impl_signaled_complete_exact=false
    local impl_signaled_complete_trailing=false

    if [[ "$trimmed_result" == "<promise>COMPLETE</promise>" ]]; then
      impl_signaled_complete_exact=true
    fi

    if [[ "$completion_mode" == "trailing" && "$trimmed_last_line" == "<promise>COMPLETE</promise>" ]]; then
      impl_signaled_complete_trailing=true
    fi

    if [[ "$result" == *"<promise>COMPLETE</promise>"* ]]; then
      local output_head
      output_head=${result//$'\n'/ }
      output_head=${output_head//$'\r'/ }
      output_head=${output_head:0:100}
      if [[ "$impl_signaled_complete_exact" != "true" && "$impl_signaled_complete_trailing" != "true" ]]; then
        ui_log "WARN" "Malformed COMPLETE output - token found but not accepted"
        report_event "WARN_MALFORMED_COMPLETE" "$i" "" "" "" "" "$ITER_LOG_PATH" "output_head=$output_head"
      fi
    fi

    printf '%s\n' "$result"

    # Exit on non-zero claude exit
    if ((claude_exit != 0)); then
      show_run_summary "claude_failed" "$claude_exit"
      [[ "$summary_json" == "true" ]] && write_summary_json "claude_failed" "$claude_exit"
      run_postmortem "claude_failed" || true
      show_completion_screen "$no_wait"
      exit "$claude_exit"
    fi

    # Verification gate (if configured). If it fails, skip reviewer and keep looping.
    if [[ -n "$verify_cmds" ]]; then
      if ! maybe_run_verification_phase "$i"; then
        ui_log "WARN" "Verification failed; skipping review and retrying next iteration"
        continue
      fi
    fi

    # Run reviewer phase if enabled
    if [[ "$reviewer" == "true" ]]; then
      local reviewer_exit=0

      local head_before=""
      local head_after=""
      if git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
        head_before=$(git rev-parse HEAD 2>/dev/null || true)
      fi

      run_reviewer_phase "$spec_path" "$plan_path" "$model" "$i" || reviewer_exit=$?

      if git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
        head_after=$(git rev-parse HEAD 2>/dev/null || true)
      fi

      if [[ -n "$head_before" && -n "$head_after" && "$head_before" != "$head_after" ]]; then
        maybe_run_verification_phase "${i}R" || true
      fi

      # Reviewer signaled full completion (all tasks [R] and no [ ] remaining)
      if ((reviewer_exit == 42)); then
        if [[ -n "$verify_cmds" ]]; then
          if ! maybe_run_verification_phase "${i}FINAL"; then
            ui_log "WARN" "Final verification failed; continuing"
            continue
          fi
        fi

        record_completion "$i" "reviewer"
        show_run_summary "complete_reviewer" "0"
        [[ "$summary_json" == "true" ]] && write_summary_json "complete_reviewer" "0"
        run_postmortem "complete_reviewer" || true
        show_completion_screen "$no_wait"
        printf '<promise>COMPLETE</promise>\n'
        exit 0
      fi
    fi

    # Completion gating: trust plan state (and reviewer backlog) over agent output.
    pending_tasks=$(count_plan_pending_tasks "$plan_path")
    pending_review=$(count_plan_items_to_review "$plan_path")

    if ((pending_tasks == 0)); then
      if [[ "$reviewer" == "true" && $pending_review -gt 0 ]]; then
        ui_log "WARN" "Plan complete but review backlog remains ($pending_review items)"
      else
        if [[ -n "$verify_cmds" ]]; then
          if ! maybe_run_verification_phase "${i}FINAL"; then
            ui_log "WARN" "Final verification failed; continuing"
            continue
          fi
        fi

        record_completion "$i" "plan"
        show_run_summary "complete_plan" "0"
        [[ "$summary_json" == "true" ]] && write_summary_json "complete_plan" "0"
        run_postmortem "complete_plan" || true
        show_completion_screen "$no_wait"
        printf '<promise>COMPLETE</promise>\n'
        exit 0
      fi
    fi

    # If reviewer is disabled, allow agent-protocol completion (but still require plan to be done).
    if [[ "$reviewer" != "true" ]]; then
      if [[ "$impl_signaled_complete_exact" == "true" || "$impl_signaled_complete_trailing" == "true" ]]; then
        ui_log "WARN" "Agent signaled COMPLETE but plan still has pending tasks ($pending_tasks)"
        report_event "WARN_COMPLETE_WITH_PENDING" "$i" "" "" "" "" "$ITER_LOG_PATH" "pending=$pending_tasks"
      fi
    fi
  done

  show_run_summary "iterations_exhausted" "0"
  [[ "$summary_json" == "true" ]] && write_summary_json "iterations_exhausted" "0"
  run_postmortem "iterations_exhausted" || true
  show_completion_screen "$no_wait"
}

if [[ "${BASH_SOURCE[0]}" == "$0" ]]; then
  main "$@"
fi
